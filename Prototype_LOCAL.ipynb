{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, udf\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, monotonically_increasing_id\n",
    "from pyspark.sql.types import TimestampType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = 'unzipped_data/'\n",
    "output_data = 'output/'\n",
    "\n",
    "def process_song_data(spark=spark, input_data=input_data, output_data=output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table (song_id, title, artist_id, year, duration)\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy('year', 'artist_id').mode('overwrite').parquet(output_data + 'songs')\n",
    "\n",
    "    # extract columns to create artists table(artist_id, name, location, lattitude, longitude)\n",
    "    artists_table = df.select('artist_id', col('artist_name').alias('name'), col('artist_location').alias('location'), col('artist_latitude').alias('latitude'), col('artist_longitude').alias('longitude')).distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode('overwrite').parquet(output_data + 'artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_song_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = 'unzipped_data/'\n",
    "output_data = 'output/'\n",
    "\n",
    "def process_log_data(spark = spark, input_data = input_data, output_data = output_data):\n",
    "\n",
    "    # read log data file\n",
    "    df_log = spark.read.json(input_data + 'log_data/*.json')\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df_log = df_log.filter(df_log.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df_log.select(col('userId').alias('user_id'), col('firstName').alias('first_name'), col('lastName').alias('last_name'), 'gender', 'level').distinct()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode('overwrite').parquet(output_data + 'users')\n",
    "\n",
    "    # create timestamp column from original timestamp column in datetime format\n",
    "    # neccessary to define the return type of udf as timestamp type\n",
    "    get_timestamp = udf(lambda ts: datetime.fromtimestamp(ts/1000),TimestampType())\n",
    "    df_log = df_log.withColumn('start_time', get_timestamp(df_log.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df_log.select('start_time') \\\n",
    "                    .withColumn('hour', hour('start_time')) \\\n",
    "                    .withColumn('day', dayofmonth('start_time')) \\\n",
    "                    .withColumn('week', weekofyear('start_time')) \\\n",
    "                    .withColumn('month', month('start_time'))\\\n",
    "                    .withColumn('year', year('start_time')) \\\n",
    "                    .withColumn('weekday', dayofweek('start_time'))\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'time')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    df_song = spark.read.json(input_data + 'song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    \n",
    "    songplays_table = df_log.join(df_song, (df_log.song == df_song.title)\\\n",
    "                                        & (df_log.artist == df_song.artist_name)\\\n",
    "                                        & (df_log.length == df_song.duration), \"inner\") \\\n",
    "                            .distinct() \\\n",
    "                            .select('start_time', col('userId').alias('user_id'), 'level', 'song_id', \\\n",
    "                                    'artist_id', col('sessionId').alias('session_id'),'location', col('userAgent').alias('user_agent')) \\\n",
    "                            .withColumn(\"songplay_id\", monotonically_increasing_id()).withColumn('year', year('start_time')).withColumn('month', month('start_time'))\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'songplays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_log_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = \"song_data/*/*/*/*.json\"\n",
    "    \n",
    "# read song data file\n",
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|  artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                 |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|      Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|     New York, NY|       -74.00712|            Tiny Tim| 43.36281|        1|SOAOIBZ12AB01815BE|I Hold Your Hand ...|2000|\n",
      "|ARDNS031187B9924F0|       32.67828|          Georgia|       -83.22295|          Tim Wilson|186.48771|        1|SONYPOM12A8C13B2D7|I Think My Wife I...|2005|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create songs table (song_id, title, artist_id, year, duration)\n",
    "songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.partitionBy('year', 'artist_id').mode('overwrite').parquet('output/songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create artists table(artist_id, name, location, lattitude, longitude)\n",
    "artists_table = df.select('artist_id', col('artist_name').alias('name'), col('artist_location').alias('location'), col('artist_latitude').alias('latitude'), col('artist_longitude').alias('longitude')).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+---------------+--------+----------+\n",
      "|         artist_id|           name|       location|latitude| longitude|\n",
      "+------------------+---------------+---------------+--------+----------+\n",
      "|ARPBNLO1187FB3D52F|       Tiny Tim|   New York, NY|40.71455| -74.00712|\n",
      "|ARXR32B1187FB57099|            Gob|               |    null|      null|\n",
      "|AROGWRA122988FEE45|Christos Dantis|               |    null|      null|\n",
      "|ARBGXIG122988F409D|     Steel Rain|California - SF|37.77916|-122.42005|\n",
      "|AREVWGE1187B9B890A|     Bitter End|      Noci (BA)| -13.442|  -41.9952|\n",
      "+------------------+---------------+---------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.mode('overwrite').parquet('output/artists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = 'unzipped_data/'\n",
    "output_data = 'output/'\n",
    "\n",
    "def process_log_data(spark = spark, input_data = input_data, output_data = output_data):\n",
    "\n",
    "    # read log data file\n",
    "    df_log = spark.read.json(input_data + 'log_data/*.json')\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df_log = df_log.filter(df_log.page == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df_log.select(col('userId').alias('user_id'), col('firstName').alias('first_name'), col('lastName').alias('last_name'), 'gender', 'level').distinct()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode('overwrite').parquet(output_data + 'users')\n",
    "\n",
    "    # create timestamp column from original timestamp column in datetime format\n",
    "    # neccessary to define the return type of udf as timestamp type\n",
    "    get_datetime = udf(lambda ts: datetime.fromtimestamp(ts/1000),TimestampType())\n",
    "    df_log = df_log.withColumn('start_time', get_datetime(df_log.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df_log.select('start_time') \\\n",
    "                    .withColumn('hour', hour('start_time')) \\\n",
    "                    .withColumn('day', dayofmonth('start_time')) \\\n",
    "                    .withColumn('week', weekofyear('start_time')) \\\n",
    "                    .withColumn('month', month('start_time'))\\\n",
    "                    .withColumn('year', year('start_time')) \\\n",
    "                    .withColumn('weekday', dayofweek('start_time'))\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'time')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    df_song = spark.read.json(input_data + 'song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    \n",
    "    songplays_table = df_log.join(df_song, (df_log.song == df_song.title)\\\n",
    "                                        & (df_log.artist == df_song.artist_name)\\\n",
    "                                        & (df_log.length == df_song.duration), \"inner\") \\\n",
    "                            .distinct() \\\n",
    "                            .select('start_time', col('userId').alias('user_id'), 'level', 'song_id', \\\n",
    "                                    'artist_id', col('sessionId').alias('session_id'),'location', col('userAgent').alias('user_agent')) \\\n",
    "                            .withColumn(\"songplay_id\", monotonically_increasing_id()).withColumn('year', year('start_time')).withColumn('month', month('start_time'))\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'songplays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "process_log_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
